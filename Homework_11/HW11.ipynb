{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fe41cb",
   "metadata": {},
   "source": [
    "# Homework 11\n",
    "\n",
    "## 2 Assignment Transformers\n",
    "\n",
    "Task: implement a Transformer architecture model (instead of an RNN model) that predicts a categorical distribution over possible next tokens such that sampling from this distribution leads to plausible next tokens. \n",
    "Implement a decoder-block based generative language model in order to use its autoregressive property to train it on prediction errors of all tokens in the input sequence. \n",
    "\n",
    "The model will take a fixed number of input tokens from a text and predict the distribution over the vocabulary for the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881e9ac",
   "metadata": {},
   "source": [
    "## 2.1 Dataset, preprocessing and tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c8a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_txt\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import tqdm\n",
    "import sentencepiece as sp\n",
    "import io\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73e6bf",
   "metadata": {},
   "source": [
    "Dataset of choice: Harry Potter Book 1 (downloaded from https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051eae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the txt file\n",
    "hp_raw = open(\"Harry_Potter_1_Sorcerers_Stone.txt\", \"r\")  \n",
    "# read file\n",
    "data = hp_raw.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7957beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "data = data.lower()\n",
    "# delete special characters, only alphanumeric values and white space/linebreaks remain\n",
    "# (we keep whitespace/linebreaks for the tokenizer later)\n",
    "data = re.sub(\"['.,;\\-!?%$\\\"]\", \"\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd90065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harry potter and the sorcerers stone\\n\\n\\nchapter one\\n\\nthe boy who lived\\n\\nmr and mrs dursley of number '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bbf3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new txt file with preprocessed harry potter text for tokenizer\n",
    "f = open(\"harrypotter.txt\", \"w\")\n",
    "f.write(data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812c7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: vocabulary size\n",
    "VOCAB_SIZE = 4242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1f698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizer on preprocessed harry potter text\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input='harrypotter.txt', model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8428c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deserialize the trained model file to load it in the correct format\n",
    "trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
    "\n",
    "# load the model as a tokenizer that can be used inside a tensorflow model\n",
    "tokenizer = tf_txt.SentencepieceTokenizer(\n",
    "    model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
    "    add_bos=False, add_eos=False, return_nbest=False, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf2aa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([226  82 980], shape=(3,), dtype=int32)\n",
      "tf.Tensor(b'magic is real', shape=(), dtype=string)\n",
      "tf.Tensor([ 15  85   6 280  10], shape=(5,), dtype=int32)\n",
      "tf.Tensor(b'you are a wizard harry', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "tokens = tokenizer.tokenize(\"magic is real\")\n",
    "print(tokens)\n",
    "print(tokenizer.detokenize(tokens))\n",
    "# because it's fun\n",
    "tokens = tokenizer.tokenize(\"you are a wizard harry\")\n",
    "print(tokens)\n",
    "print(tokenizer.detokenize(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4360538",
   "metadata": {},
   "source": [
    "We want to have input sequences of length m tokens (m should be between 32 and 256 - here: seq_length); for this we use tf text.sliding window and pass the tokenized text and the width m + 1 as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356b1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: sequence length\n",
    "SEQ_LENGTH = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b3e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read harry potter file\n",
    "hp = open(\"harrypotter.txt\", \"r\")  \n",
    "data = hp.read()  \n",
    "# tokenize\n",
    "tokenized_data = tokenizer.tokenize(data)\n",
    "# get sequence windows of size = seq_length\n",
    "sequences = tf_txt.sliding_window(tokenized_data, width=SEQ_LENGTH + 1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8280d49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([86307, 143])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6666e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset out of sequences\n",
    "hp_ds = tf.data.Dataset.from_tensor_slices(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9135102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(143,), dtype=int32, numpy=\n",
       "array([  10,  134,    4,    3,  725,    8,  171,  738,  372,   45,    3,\n",
       "        157,   78, 1274,  159,    4,  294,  239,    9,  653,  341,  771,\n",
       "        523,   37, 1317,    5,  168,   23,   24,   37, 1939, 1134, 1034,\n",
       "         15,   79,  173,   24,   37,    3,  153,  132,   15,   41,  758,\n",
       "          5,   31, 1831,   14,  183,  445,  116, 1467,  155,   24,   73,\n",
       "         68,  906,   30,  597, 2131,  159,  239,   11,    3, 1436, 1020,\n",
       "          9,    6, 3045,  302,  672,  790,    8,  148,  203, 1629,    7,\n",
       "         11,    6,  428, 2597,   46,  297,   30,  576,  192,  676,  424,\n",
       "        976,    7,  126,   40,    6,   79,  247, 1498,  294,  239,   11,\n",
       "       1112,    4, 1915,  161,    4,   19,  357,  815,    3,  454,    6,\n",
       "       3865,    9,  676,  148,  167,   14,   79, 1673,   26,   47, 1071,\n",
       "         48,  173,    9,   74,  104, 2713,   16,   72, 1458,  452, 2015,\n",
       "          8,  507, 1980,   21,    3, 1587, 2488, 1793,    8,    3,  257])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of one datapoint = one sequence\n",
    "iterator = iter(hp_ds)\n",
    "iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bbd0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of the sequence with length m+1, the first m tokens are the inputs and the last token is the target\n",
    "hp_ds = hp_ds.map(lambda seq: tf.split(sequences, [SEQ_LENGTH, 1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c31c22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(86307, 142), dtype=int32, numpy=\n",
       " array([[  10,  134,    4, ..., 1793,    8,    3],\n",
       "        [ 134,    4,    3, ...,    8,    3,  257],\n",
       "        [   4,    3,  725, ...,    3,  257,   19],\n",
       "        ...,\n",
       "        [  10,  827,   16, ...,   30,  105,   49],\n",
       "        [ 827,   16,   56, ...,  105,   49, 1330],\n",
       "        [  16,   56,  272, ...,   49, 1330,    3]])>,\n",
       " <tf.Tensor: shape=(86307, 1), dtype=int32, numpy=\n",
       " array([[ 257],\n",
       "        [  19],\n",
       "        [   6],\n",
       "        ...,\n",
       "        [1330],\n",
       "        [   3],\n",
       "        [ 243]])>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of one datapoint = one sequence (input tokens + target token)\n",
    "iterator = iter(hp_ds)\n",
    "iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47a9645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: batch size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bb7ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle, batch, prefetch\n",
    "hp_ds = hp_ds.cache().shuffle(1000).batch(BATCH_SIZE).prefetch(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374270f6",
   "metadata": {},
   "source": [
    "## 2.2 The Model Components\n",
    "\n",
    "\n",
    "### 2.2.1 The Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3623e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: embedding dimensionality\n",
    "# somethingbetween 64 and 256\n",
    "EMBED_DIM = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b5ac122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # each input token index is mapped to a vector that is looked up from a table\n",
    "        self.embed_token = tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "        # positional embedding \n",
    "        self.embed_pos = tf.keras.layers.Embedding(SEQ_LENGTH, EMBED_DIM)\n",
    "        \n",
    "    def call(self, token_seq):\n",
    "        # indices to look up the positional code for each sub-word\n",
    "        indices = tf.range(0, SEQ_LENGTH)\n",
    "        # feed into embedding layers\n",
    "        token_embed = self.embed_token(token_seq)\n",
    "        idx_embed = self.embed_pos(indices)\n",
    "        # concatenate ?\n",
    "        seq_embed = token_embed + idx_embed\n",
    "        \n",
    "        return seq_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a4405",
   "metadata": {},
   "source": [
    "### 2.2.2 The TransformerBlock Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e69bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=3, key_dim=EMBED_DIM)\n",
    "        self.dense1 = tf.keras.layers.Dense(142, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(EMBED_DIM)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.attention(query=inputs, value=inputs, use_causal_mask=True)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = inputs + x\n",
    "        x = self.layernorm1(x)\n",
    "        ln_out = x\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = x + ln_out\n",
    "        x = layernorm2(x)\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e91e9f",
   "metadata": {},
   "source": [
    "### 2.2.3 The subclassed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8083a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.embed_layer = Embedding()\n",
    "        self.transf_blocks = TransformerBlock()\n",
    "        self.dense = tf.keras.layers.Dense(VOCAB_SIZE)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.metrics_list = [\n",
    "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                        tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
    "                        tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\")\n",
    "                       ]        \n",
    "        \n",
    "    @tf.function \n",
    "    def call(self, data):\n",
    "        x = self.embed_layer(data)\n",
    "        x = self.transf_blocks(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "          \n",
    "    def reset_metrics(self): \n",
    "        for metric in self.metrics_list:\n",
    "            metric.reset_states()\n",
    "          \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(data, training=True)\n",
    "            loss = self.loss_function(data, predictions) + tf.reduce_sum(self.losses)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update loss metric\n",
    "        self.metrics_list[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics_list[1:]:\n",
    "            metric.update_state(data,predictions)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "        \n",
    "        \n",
    "    # TO DO - I don't really understand the single steps in the hw description ...\n",
    "    # method receives a text prompt and the desired output length and returns a continuation of the prompt of a specified length  \n",
    "    def generate_text(self, prompt, output_length, top_k):\n",
    "        \n",
    "        # repeat until desired output length is achieved\n",
    "        for i in range(len(output_length)):\n",
    "            \n",
    "            # tokenize text input (=prompt)\n",
    "            tokenized_prompt = self.tokenizer.tokenize(prompt)\n",
    "            # add an extra batch dimension\n",
    "            text_input = tf.expand_dims(tokenized_prompt)\n",
    "            # pad?\n",
    "            text_input = tf.pad(text_input)\n",
    "            # feed into model?\n",
    "            output = self.call(text_input)\n",
    "            # returns two tensors, one with the top k highest logits and another with the corresponding token indices\n",
    "            vals, idx = tf.math.top_k(output, k=top_k, sorted=True)\n",
    "            # sample one token from the top k distribution\n",
    "            # this can be done with tf.random.categorical on the last time-step in the sequence of logits that your model outputs\n",
    "            # ? \n",
    "            sample = tf.random.categorical(vals, 1)\n",
    "            # index the tensor with the corresponding token index (using the sampled index to index the tensor that contains the corresponding token indices)\n",
    "            # ?\n",
    "            new_token = idx[sample]\n",
    "            # concatenate the token to the sequence\n",
    "            new_text = text_input + new_token\n",
    "            # if necessary truncate the length of the input (e.g. by indexing with [-self.max_len:]), and repeat until the desired length is reached.\n",
    "\n",
    "        \n",
    "        # detokenize \n",
    "        new_text = self.tokenizer.detokenize(new_text)\n",
    "        return new_text\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89aa230",
   "metadata": {},
   "source": [
    "## 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cadefa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save the log\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "705ae695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, prompt, train_ds, epochs, train_summary_writer, val_summary_writer):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        \n",
    "        # Training:\n",
    "        \n",
    "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
    "            metrics = model.train_step(data)\n",
    "            \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        \n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "    \n",
    "        model.reset_metrics()    \n",
    "        \n",
    "        # Validation:\n",
    "        # test the text generator\n",
    "        # ? \n",
    "        gen_text = model.generate_text(prompt, 5, 3)\n",
    "        print(gen_text)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # why do we need validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7738b8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14956), started 0:29:27 ago. (Use '!kill 14956' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ff0f8be581122505\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ff0f8be581122505\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2883bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: number of epochs\n",
    "# between 100 and 600 epochs depending on the text used\n",
    "NUM_EPOCHS = 420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e86d76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = TransformerModel(tokenizer)\n",
    "starting_prompt = 'Hogwards is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfc8bcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2698 [00:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[86307,142] and type int32 on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node split}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# run the training loop \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstarting_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#val_ds=val_ds, # to do?\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_summary_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_summary_writer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mval_summary_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_summary_writer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, prompt, train_ds, epochs, train_summary_writer, val_summary_writer)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Training:\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_ds, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      9\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain_step(data)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# logging the validation metrics to the log file which is used by tensorboard\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    765\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 749\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3014\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3016\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3018\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[86307,142] and type int32 on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node split}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# run the training loop \n",
    "training_loop(model=model, \n",
    "                prompt = starting_prompt,\n",
    "                train_ds=hp_ds, \n",
    "                #val_ds=val_ds, # to do?\n",
    "                epochs=NUM_EPOCHS, \n",
    "                train_summary_writer=train_summary_writer, \n",
    "                val_summary_writer=val_summary_writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "iannwtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
