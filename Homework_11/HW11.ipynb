{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fe41cb",
   "metadata": {},
   "source": [
    "# Homework 11\n",
    "\n",
    "## 2 Assignment Transformers\n",
    "\n",
    "Task: implement a Transformer architecture model (instead of an RNN model) that predicts a categorical distribution over possible next tokens such that sampling from this distribution leads to plausible next tokens. \n",
    "Implement a decoder-block based generative language model in order to use its autoregressive property to train it on prediction errors of all tokens in the input sequence. \n",
    "\n",
    "The model will take a fixed number of input tokens from a text and predict the distribution over the vocabulary for the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881e9ac",
   "metadata": {},
   "source": [
    "## 2.1 Dataset, preprocessing and tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c8a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_txt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, TimeDistributed, LSTM, GlobalAvgPool2D, AbstractRNNCell, MaxPooling2D, RNN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import tqdm\n",
    "import sentencepiece as sp\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73e6bf",
   "metadata": {},
   "source": [
    "Dataset of choice: Harry Potter Book 1 (downloaded from https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051eae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the txt file\n",
    "hp_raw = open(\"Harry_Potter_1_Sorcerers_Stone.txt\", \"r\")  \n",
    "# read file\n",
    "data = hp_raw.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7957beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "data = data.lower()\n",
    "# delete special characters, only alphanumeric values and white space/linebreaks remain\n",
    "# (we keep whitespace/linebreaks for the tokenizer later)\n",
    "data = re.sub(\"['.,;\\-!?%$\\\"]\", \"\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd90065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harry potter and the sorcerers stone\\n\\n\\nchapter one\\n\\nthe boy who lived\\n\\nmr and mrs dursley of number '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bbf3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new txt file with preprocessed harry potter text for tokenizer\n",
    "f = open(\"harrypotter.txt\", \"w\")\n",
    "f.write(data)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812c7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: vocabulary size\n",
    "VOCAB_SIZE = 4242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1f698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizer on preprocessed harry potter text\n",
    "sp.SentencePieceTrainer.train(\n",
    "    input='harrypotter.txt', model_prefix='tokenizer_model', model_type=\"unigram\", vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8428c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deserialize the trained model file to load it in the correct format\n",
    "trained_tokenizer_model = tf.io.gfile.GFile('tokenizer_model.model', \"rb\").read()\n",
    "\n",
    "# load the model as a tokenizer that can be used inside a tensorflow model\n",
    "tokenizer = tf_txt.SentencepieceTokenizer(\n",
    "    model=trained_tokenizer_model, out_type=tf.int32, nbest_size=-1, alpha=1, reverse=False,\n",
    "    add_bos=False, add_eos=False, return_nbest=False, name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daf2aa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([226  82 980], shape=(3,), dtype=int32)\n",
      "tf.Tensor(b'magic is real', shape=(), dtype=string)\n",
      "tf.Tensor([ 15  85   6 280  10], shape=(5,), dtype=int32)\n",
      "tf.Tensor(b'you are a wizard harry', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "tokens = tokenizer.tokenize(\"magic is real\")\n",
    "print(tokens)\n",
    "print(tokenizer.detokenize(tokens))\n",
    "# because it's fun\n",
    "tokens = tokenizer.tokenize(\"you are a wizard harry\")\n",
    "print(tokens)\n",
    "print(tokenizer.detokenize(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4360538",
   "metadata": {},
   "source": [
    "We want to have input sequences of length m tokens (m should be between 32 and 256 - here: seq_length); for this we use tf text.sliding window and pass the tokenized text and the width m + 1 as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356b1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: sequence length\n",
    "seq_length = 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b3e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read harry potter file\n",
    "hp = open(\"harrypotter.txt\", \"r\")  \n",
    "data = hp.read()  \n",
    "# tokenize\n",
    "tokenized_data = tokenizer.tokenize(data)\n",
    "# get sequence windows of size = seq_length\n",
    "sequences = tf_txt.sliding_window(tokenized_data, width=seq_length + 1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8280d49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([86302, 143])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6666e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset out of sequences\n",
    "hp_ds = tf.data.Dataset.from_tensor_slices(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9135102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(143,), dtype=int32, numpy=\n",
       "array([  10,  134,    4,    3,  725,    8,  171,  738,  372,   45,    3,\n",
       "        157,   78, 1274,  159,    4,  294,  239,    9,  653,  341,  771,\n",
       "        523,   37, 1317,    5,  168,   23,   24,   37, 1939, 1134, 1034,\n",
       "         15,   79,  173,   24,   37,    3,  153,  132,   15,   41,  758,\n",
       "          5,   31, 1831,   14,  183,  445,  116, 1467,  155,   24,   73,\n",
       "         68,  906,   30,  597, 2131,  159,  239,   11,    3, 1436, 1020,\n",
       "          9,    6, 3045,  302,  672,  790,    8,  148,  203, 1629,    7,\n",
       "         11,    6,  428, 2597,   46,  297,   30,  576,  192,  676,  424,\n",
       "        976,    7,  126,   40,    6,   79,  247, 1498,  294,  239,   11,\n",
       "       1112,    4, 1915,  161,    4,   19,  357,  815,    3,  454,    6,\n",
       "       3865,    9,  676,  148,  167,   14,   79, 1673,   26,   47, 1071,\n",
       "         48,  173,    9,   74,  104, 2713,   16,   72, 1458,  452, 2015,\n",
       "          8,  507, 1980,   21,    3, 1587, 2488, 1793,    8,    3,  257])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of one datapoint = one sequence\n",
    "iterator = iter(hp_ds)\n",
    "iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bbd0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of the sequence with length m+1, the first m tokens are the inputs and the last token is the target\n",
    "hp_ds = hp_ds.map(lambda seq: tf.split(sequences, [seq_length, 1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c31c22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(86302, 142), dtype=int32, numpy=\n",
       " array([[  10,  134,    4, ..., 1793,    8,    3],\n",
       "        [ 134,    4,    3, ...,    8,    3,  257],\n",
       "        [   4,    3,  725, ...,    3,  257,   19],\n",
       "        ...,\n",
       "        [   9,   10,  827, ...,   30,  105,   49],\n",
       "        [  10,  827,   16, ...,  105,   49, 1330],\n",
       "        [ 827,   16,   56, ...,   49, 1330,    3]])>,\n",
       " <tf.Tensor: shape=(86302, 1), dtype=int32, numpy=\n",
       " array([[ 257],\n",
       "        [  19],\n",
       "        [   6],\n",
       "        ...,\n",
       "        [1330],\n",
       "        [   3],\n",
       "        [ 243]])>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of one datapoint = one sequence (input tokens + target token)\n",
    "iterator = iter(hp_ds)\n",
    "iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47a9645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter: batch size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bb7ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle, batch, prefetch\n",
    "hp_ds = hp_ds.cache().shuffle(1000).batch(BATCH_SIZE).prefetch(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374270f6",
   "metadata": {},
   "source": [
    "## 2.2 The Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ac122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5280274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "iannwtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
