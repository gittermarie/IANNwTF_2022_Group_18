{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3b2c0e",
   "metadata": {},
   "source": [
    "# Homework 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99216cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berit\\AppData\\Local\\Continuum\\miniconda3\\envs\\iannwtf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, TimeDistributed, LSTM, GlobalAvgPool2D, AbstractRNNCell, MaxPooling2D, RNN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tqdm\n",
    "# magic line only needed in jupyter notebooks!\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e149696",
   "metadata": {},
   "source": [
    "### Dowloading Data for the Task from the Quick, Draw! Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8dd519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "#import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237a96af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'aircraft carrier', b'airplane', b'alarm clock', b'ambulance', b'angel', b'animal migration', b'ant', b'anvil', b'apple', b'arm']\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "categories = [line.rstrip(b'\\n') for line in urllib.request.urlopen('https://raw.githubusercontent.com/googlecreativelab/quickdraw-dataset/master/categories.txt')]\n",
    "print(categories[:10])\n",
    "category = 'candle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f5a81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141545 images to train on\n"
     ]
    }
   ],
   "source": [
    "# skip this cell if you already downloaded the data/you have the folder containing the data\n",
    "\n",
    "# Creates a folder to download the original drawings into.\n",
    "# We chose to use the numpy format : 1x784 pixel vectors, with values going from 0 (white) to 255 (black). We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]\n",
    "# We reshape them later to 28x28 grids and normalize the pixel intensity to [-1, 1]\n",
    "\n",
    "if not os.path.isdir('npy_files'):\n",
    "    os.mkdir('npy_files')\n",
    "    \n",
    "url = f'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/{category}.npy'  \n",
    "urllib.request.urlretrieve(url, f'npy_files/{category}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1842cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141545 images to train on\n"
     ]
    }
   ],
   "source": [
    "# load images from newly created local folder\n",
    "images = np.load(f'npy_files/{category}.npy')\n",
    "print(f'{len(images)} images to train on')\n",
    "\n",
    "# amount of images for training\n",
    "train_images = images[:10000]\n",
    "# subset of the images for testing\n",
    "test_images = images[15000:17500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8d4f7",
   "metadata": {},
   "source": [
    "## Assignment 2: GANs\n",
    "\n",
    "### 2.1 Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a922550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(raw_data):\n",
    "    # create tf dataset out of numpy arrays\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(raw_data)\n",
    "    # reshap from (1, 784) to shape (28,28,1)\n",
    "    dataset = dataset.map(lambda img: tf.reshape(img, (28,28,1)))\n",
    "    # normalization of tensors\n",
    "    dataset = dataset.map(lambda img: tf.image.per_image_standardization(img))\n",
    "    # cache, shuffle, batch, prefetch\n",
    "    dataset = dataset.cache().shuffle(1000).batch(32).prefetch(20)\n",
    "    \n",
    "    return dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea11d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = data_preprocessing(train_images)\n",
    "test_ds = data_preprocessing(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09596c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape of the data   \n",
    "list(train_ds.as_numpy_iterator())[1]\n",
    "np.shape(list(train_ds.as_numpy_iterator())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10996587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_image(dataset):\n",
    "    # get an image from the dataset\n",
    "    img = next(iter(dataset))\n",
    "\n",
    "    # get the first image from the batch\n",
    "    img = img[0]\n",
    "\n",
    "    # Remove the extra dimension\n",
    "    #img = tf.squeeze(img, axis=-1)\n",
    "\n",
    "    # plot the image\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9db800a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize how a data point looks like \n",
    "print_image(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6057f",
   "metadata": {},
   "source": [
    "### 2.2 The GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2200c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \n",
    "    # perform downsampling and output a vector of probabilities indicating whether the input was fake or real\n",
    "    # basically just like a CNN architecture for a binary classification task\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # list of all layers to iterate through during call\n",
    "        self.layer_list = []\n",
    "        \n",
    "        # layers\n",
    "        self.layer_list.append(Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.layer_list.append(tf.keras.layers.BatchNormalization())\n",
    "        #self.layer_list.append(tf.keras.layers.Dropout(0.2))\n",
    "        self.layer_list.append(Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.layer_list.append(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "        self.layer_list.append(Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.layer_list.append(tf.keras.layers.BatchNormalization())\n",
    "        #self.layer_list.append(tf.keras.layers.Dropout(0.2))\n",
    "        self.layer_list.append(Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.layer_list.append(GlobalAvgPool2D())\n",
    "        \n",
    "        # a Dense layer with a single neuron in the end\n",
    "        self.layer_list.append(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    # call funtion\n",
    "    # training flag to use certain optimization and regularization techniques -> BatchNorm and Dropout\n",
    "    @tf.function\n",
    "    def call(self, x, training = False):\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5bf66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \n",
    "    # takes a random point from the latent space and returns a generated image\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # list of all layers to iterate through during call\n",
    "        self.layer_list = []\n",
    "        \n",
    "        # layers\n",
    "        # latent space (size adjustable, but start with 100) comes as a 1d vector\n",
    "        self.layer_list.append(Dense(64, activation='relu', input_shape=(100,)))\n",
    "        # reshape such that we have a 2D image\n",
    "        self.layer_list.append(tf.keras.layers.Reshape((8,8,2)))\n",
    "        \n",
    "        # convolutional layers with upsampling in between and batch normalization/dropout\n",
    "        self.layer_list.append(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.layer_list.append(tf.keras.layers.BatchNormalization())\n",
    "        #self.layer_list.append(tf.keras.layers.Dropout(0.2))\n",
    "        # upsampling technique (Conv2DTranspose) - even-sized kernels in transposed convolutions\n",
    "        self.layer_list.append(tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "        self.layer_list.append(Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.layer_list.append(tf.keras.layers.BatchNormalization())\n",
    "        #self.layer_list.append(tf.keras.layers.Dropout(0.2))\n",
    "        \n",
    "        # in the last layer, perform a convolution with 1 feature map and tanh activation\n",
    "        self.layer_list.append(Conv2D(filters=1, kernel_size=3, padding='same', activation='tanh'))\n",
    "          \n",
    "        \n",
    "    # call funtion\n",
    "    # training flag to use certain optimization and regularization techniques -> BatchNorm and Dropout\n",
    "    @tf.function\n",
    "    def call(self, x, training = False):\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c93338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        # optimzer, metrics, loss\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        self.metrics_list = [\n",
    "                        tf.keras.metrics.Mean(name=\"generator_loss\"),\n",
    "                        tf.keras.metrics.Mean(name=\"discriminator_loss\"),\n",
    "                        tf.keras.metrics.BinaryAccuracy(name=\"discriminator_acc\") \n",
    "                       ]\n",
    "\n",
    "        self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "        self.discriminator = Discriminator() \n",
    "        self.generator = Generator() \n",
    "\n",
    "        \n",
    "    # call funtion\n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        x = self.generator(x) \n",
    "        x = self.discriminator(x)\n",
    "        return x\n",
    "    \n",
    "    # metrics property\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "    # reset all metrics objects\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "    \n",
    "    # train step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        # create random noise = input to generator; size of noise vector is the size of latent space \n",
    "        noise = tf.random.normal((100,)) #????\n",
    "        \n",
    "        # calculate and backpropagate gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # discriminator sees a batch of true images and a batch of the generated images\n",
    "            generated_images = self.generator(noise, training =True)\n",
    "            true_images = data\n",
    "            # predictions\n",
    "            output_generated = self.discriminator(generated_images, training=True)\n",
    "            output_true = self.discriminator(true_images, training=True)\n",
    "            # losses \n",
    "            # TO DO !!!!!!!!!!\n",
    "            '''\n",
    "            Compute the binary cross entropy between the generator’s output on fake images and all labels = 0. \n",
    "            Similarly, compute the BCE between the generator’s output on the real images and all labels = 1. \n",
    "            Add them both to receive the resulting loss of the discriminator.\n",
    "            tf.ones like() and tf.zeros like() could be helpful for creating the true labels as a comparison.\n",
    "            \n",
    "            '''\n",
    "            # loss of the discriminator is based on how well the discriminator detected fake images as fake and real images as real\n",
    "            loss_discriminator =\n",
    "            # loss of the generator is estimated by how well the generator was able to fool the discriminator\n",
    "            loss_generator = \n",
    "            \n",
    "            # accuracy of discriminator? \n",
    "            acc_discriminator = \n",
    "            \n",
    "        \n",
    "        # calculate gradients for both based on respective loss\n",
    "        gradients_generator = tape.gradient(loss_generator, self.trainable_variables)\n",
    "        gradients_discriminator = tape.gradient(loss_discriminator, self.trainable_variables)\n",
    "        \n",
    "        # apply optimizer for both \n",
    "        # do I need to initialize two individual optimizers before? \n",
    "        self.optimizer.apply_gradients(zip(gradients_generator, self.generator.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(gradients_discriminator, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # update metrics\n",
    "        self.metrics[0].update_state(loss_generator)\n",
    "        self.metrics[1].update_state(loss_discriminator)\n",
    "        self.metrics[2].update_state(acc_discriminator)\n",
    "        \n",
    "        # Return a dictionary mapping metric names to current value to keep track of training\n",
    "        d = {m.name: m.result() for m in self.metrics}\n",
    "        return d\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        \n",
    "        # TO DO !!!!!!!\n",
    "        \n",
    "        # same as in training but without backpropagating\n",
    "        # ? \n",
    "        # For evaluation, you should therefore create some random latent vectors before training and feed them \n",
    "        # through the generator regularly. You can plot the resulting images to evaluate training.\n",
    "        # ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a0d7b",
   "metadata": {},
   "source": [
    "### 2.3 Training\n",
    "\n",
    "* visualize your training matrices as well as generate candle images using a random seed to assert the visual quality of your GAN\n",
    "* start with 10 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82231a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "iannwtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
