TO DO:

## 5 things to optimze in our model to fight overfitting 
(see models.py file for code implementations)

   1. Batch normalization
- to make the training speed faster
- even though if we put high learning rate, optimazation works well
- weight initialization can be robust
- to prevent from overfitting issue 

   2. Dropout
- to prevent from overfitting issue 

   3. L2 Regularization
- to prevent from overfitting issue
- to improve generalization 
- having more stability than L1 Regularization 

   4. Model with Glorot Normal kernel-initialization instead of Uniform
- Xavier initialization let output to be the form of standard normal distribution

   5. Label smoothing
- to reduce over-confidence    
