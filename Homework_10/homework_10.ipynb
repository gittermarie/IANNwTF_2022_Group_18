{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203fa0ba-54b1-47c4-b806-d404f1af31e5",
   "metadata": {},
   "source": [
    "## Homework 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802b77bb-5194-4439-a150-23c91344dd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 00:05:24.990041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-09 00:05:25.107774: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-09 00:05:25.110602: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-09 00:05:25.110612: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-09 00:05:25.723818: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-09 00:05:25.723870: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-09 00:05:25.723874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# useful imports \n",
    "import tensorflow as tf\n",
    "from tensorflow_text import RegexSplitter\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, TimeDistributed, LSTM, GlobalAvgPool2D, AbstractRNNCell, MaxPooling2D, RNN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab53543-4458-4c8a-8554-b5ac721e0b1f",
   "metadata": {},
   "source": [
    "### 2.2 Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd983b4-e67a-46ff-82a3-a93fe3092748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the txt file\n",
    "bible = open(\"bible.txt\", \"r\")  \n",
    "# read file\n",
    "data = bible.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8666f99-177d-497a-a167-1a67488dd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global hyperparameters\n",
    "VOCAB_SIZE = 10000\n",
    "WINDOW_SIZE = 4\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d0257e-3b5a-4974-9e11-d7f7df9e1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case and replace new-line characters with <space>\n",
    "data = data.lower().replace(\"\\n\", \" \")\n",
    "# delete special characters, only alphanumeric values remain\n",
    "# do we care for numbers or should we also delete them?\n",
    "data = re.sub('\\W+',' ', data)\n",
    "# split on space\n",
    "tokenized_data = data.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208f6c68-8ef4-40d5-93c7-aade55ef6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(data):\n",
    "    uniqe_words = set(data)\n",
    "    occurrences = defaultdict(lambda: 0)\n",
    "    for item in tokenized_data:\n",
    "        occurrences[item] +=1\n",
    "    # sort occurences such that we can get the words with the highest counts\n",
    "    # sorted from small to high counts\n",
    "    sorted_occurrences = {k: v for k, v in sorted(occurrences.items(), key=lambda item: item[1])}\n",
    "    most_common_words = list(sorted_occurrences.keys())[(12744-VOCAB_SIZE):]\n",
    "    least_common_words = list(sorted_occurrences.keys())[:(12744-VOCAB_SIZE)+1]\n",
    "    most_common_words = most_common_words[::-1]\n",
    "    return most_common_words, least_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd924ca-1eda-46bf-9c3b-9caf3e7df274",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_c_w, l_c_w = wordcount(tokenized_data)\n",
    "\n",
    "# remove instances of least commen words from data (they will not be included??)\n",
    "for word in l_c_w:\n",
    "    data = re.sub(f' {word} ',' ', data)\n",
    "    \n",
    "# assign new tokenized data\n",
    "tokenized_data = data.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b2da7-532d-4406-8ce2-6a18db7d50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 253, 449, 3, 161, 193, 43, 43, 6, 1]\n",
      "['and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness']\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary for later purposes\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "\n",
    "for word in m_c_w:\n",
    "    vocab[word] = index\n",
    "    index += 1\n",
    "    \n",
    "example_sequence = [vocab[word] for word in tokenized_data[:10]]\n",
    "print(example_sequence)\n",
    "print(tokenized_data[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f862c-d375-4eb4-929d-ecb29ba63d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse vocabulary\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0ce9c-8432-432a-8851-0a4c4bb734ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 41, 1, 17, 46, 28, 126, 2, 5, 7, 3, 1199, 328, 292, 5, 705, 509, 168, 1, 13, 4477, 2, 203, 213, 12, 708, 11, 1, 109, 2, 53, 18, 38, 128, 1, 148, 34, 53, 1220, 1109, 53, 529, 18, 68, 28, 2, 1, 103, 4, 2, 1761, 24, 392, 9, 31, 20, 193, 31, 3, 10, 159, 139, 1, 1, 1217, 736, 1986, 19, 14, 58, 6, 2180, 72, 1436, 9, 216, 103, 100, 1, 3016, 71, 1970, 1, 280, 3883, 134, 26, 320, 9, 505, 61, 169, 31, 3, 5, 2, 102, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# to create input-target pairs (= word-contexts pairs) we use \n",
    "# tf's continuous skipgram model \n",
    "# https://www.tensorflow.org/tutorials/text/word2vec\n",
    "\n",
    "bible_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      tokenized_data,\n",
    "      vocabulary_size=VOCAB_SIZE,\n",
    "      window_size=WINDOW_SIZE,\n",
    "      negative_samples=0)\n",
    "\n",
    "bible_skip_grams = [[vocab[x[0]], vocab[x[1]]] for x in bible_skip_grams]\n",
    "inputs, targets = map(list, zip(*bible_skip_grams))\n",
    "print(targets[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f1179a-e6ee-4681-85dd-32f5f5e63ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 00:06:24.642583: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-09 00:06:24.642605: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-09 00:06:24.642627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (maries-debian): /proc/driver/nvidia/version does not exist\n",
      "2023-02-09 00:06:24.642916: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# the usual cache, shuffle, batch, prefetch\n",
    "bible_dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(1000).batch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d247d-9204-4bdc-9d19-a05c87ea07fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a69766-f8a2-4d88-a3d1-a186c17bf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, optimizer, embedding_size):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        \n",
    "        self.vocabulary_size = VOCAB_SIZE\n",
    "        self.embedding_size = embedding_size\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name = \"loss\")\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def build(self,string):\n",
    "        self.embedding_mat = self.add_weight(shape=(self.vocabulary_size, self.embedding_size),\n",
    "                                             initializer=\"uniform\",\n",
    "                                             trainable=True) \n",
    "        self.output_mat = self.add_weight(shape=(self.vocabulary_size, self.embedding_size),\n",
    "                                          initializer=\"random_normal\",\n",
    "                                          trainable=True) \n",
    "        self.output_bias = self.add_weight(shape=(self.vocabulary_size,),\n",
    "                                           initializer=\"zero\",\n",
    "                                           trainable=True)\n",
    "    def call(self, inputs):\n",
    "        target_predicted = tf.nn.embedding_lookup(params=self.embedding_mat, ids=inputs)\n",
    "        return target_predicted\n",
    "    \n",
    "    def train(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(weights=self.output_mat,\n",
    "                               biases=self.output_bias, \n",
    "                               labels=tf.expand_dims(targets,axis=1), \n",
    "                               inputs=predictions,\n",
    "                               num_sampled=1,\n",
    "                               num_classes=self.vocabulary_size))\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # update accuracy\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(targets,predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd2e0d1c-4273-497b-9ebb-1d1ec842caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_ds, epochs, summary_writer):\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch: ', epoch)\n",
    "\n",
    "        for data in train_ds:\n",
    "            metrics = model.train(data)\n",
    "\n",
    "        # log train loss\n",
    "        with summary_writer.as_default():  \n",
    "            # for scalar metrics:\n",
    "            for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=e)\n",
    "\n",
    "\n",
    "       \n",
    "        model.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f61a7fc-8435-455a-9128-e7b8d831b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_writers(config_name):\n",
    "    \n",
    "    # Define where to save the logs\n",
    "    # along with this, you may want to save a config file with the same name so you know what the hyperparameters were used\n",
    "    # alternatively make a copy of the code that is used for later reference\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "    test_log_path = f\"logs/{config_name}/{current_time}/test\"\n",
    "\n",
    "    # log writer for training metrics\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "    # log writer for test metrics\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
    "    \n",
    "    return train_summary_writer, test_summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3354b-95ef-40dd-812c-06586847b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "EPOCHS = 2\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model = SkipGramModel(optimizer, 64)\n",
    "summary_writer = create_summary_writers(\"model1\")\n",
    "training_loop(model, train_ds=bible_dataset, epochs= EPOCHS, summary_writer=summary_writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
