{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203fa0ba-54b1-47c4-b806-d404f1af31e5",
   "metadata": {},
   "source": [
    "## Homework 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "802b77bb-5194-4439-a150-23c91344dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful imports \n",
    "import tensorflow as tf\n",
    "from tensorflow_text import RegexSplitter\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, TimeDistributed, LSTM, GlobalAvgPool2D, AbstractRNNCell, MaxPooling2D, RNN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "import tqdm\n",
    "from heapq import nlargest\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab53543-4458-4c8a-8554-b5ac721e0b1f",
   "metadata": {},
   "source": [
    "### 2.2 Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd983b4-e67a-46ff-82a3-a93fe3092748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the txt file\n",
    "bible = open(\"bible.txt\", \"r\")  \n",
    "# read file\n",
    "data = bible.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8666f99-177d-497a-a167-1a67488dd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global hyperparameters\n",
    "VOCAB_SIZE = 10000\n",
    "WINDOW_SIZE = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# words from the bible corpus we want to track during training\n",
    "tracked_words= ['holy', 'water', 'wine', 'love', 'son', 'father', 'devil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d0257e-3b5a-4974-9e11-d7f7df9e1bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case and replace new-line characters with <space>\n",
    "data = data.lower().replace(\"\\n\", \" \")\n",
    "# delete special characters, only alphanumeric values remain\n",
    "# do we care for numbers or should we also delete them?\n",
    "data = re.sub('\\W+',' ', data)\n",
    "# split on space\n",
    "tokenized_data = data.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "208f6c68-8ef4-40d5-93c7-aade55ef6f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(data):\n",
    "    uniqe_words = set(data)\n",
    "    occurrences = defaultdict(lambda: 0)\n",
    "    for item in tokenized_data:\n",
    "        occurrences[item] +=1\n",
    "    # sort occurences such that we can get the words with the highest counts\n",
    "    # sorted from small to high counts\n",
    "    sorted_occurrences = {k: v for k, v in sorted(occurrences.items(), key=lambda item: item[1])}\n",
    "    most_common_words = list(sorted_occurrences.keys())[(12744-VOCAB_SIZE):]\n",
    "    least_common_words = list(sorted_occurrences.keys())[:(12744-VOCAB_SIZE)+1]\n",
    "    most_common_words = most_common_words[::-1]\n",
    "    return most_common_words, least_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd924ca-1eda-46bf-9c3b-9caf3e7df274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_c_w = most_common_words\n",
    "# l_c_w = least_common_words\n",
    "m_c_w, l_c_w = wordcount(tokenized_data)\n",
    "\n",
    "# remove instances of least commen words from data (they will not be included??)\n",
    "for word in l_c_w:\n",
    "    data = re.sub(f' {word} ',' ', data)\n",
    "    \n",
    "# assign new tokenized data\n",
    "tokenized_data = data.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086b2da7-532d-4406-8ce2-6a18db7d50e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 253, 449, 3, 161, 193, 43, 43, 6, 1]\n",
      "['and', 'the', 'earth', 'was', 'without', 'form', 'and', 'void', 'and', 'darkness']\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary for later purposes\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "\n",
    "for word in m_c_w:\n",
    "    vocab[word] = index\n",
    "    index += 1\n",
    "    \n",
    "example_sequence = [vocab[word] for word in tokenized_data[:10]]\n",
    "print(example_sequence)\n",
    "print(tokenized_data[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716f862c-d375-4eb4-929d-ecb29ba63d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse vocabulary\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb03bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words from the bible corpus we want to track during training\n",
    "tracked_words= ['holy', 'water', 'wine', 'love', 'son', 'father', 'devil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fa0ce9c-8432-432a-8851-0a4c4bb734ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143, 38, 13, 112, 125, 5471, 8, 6, 17, 5, 73, 54, 26, 14, 287, 9, 1, 215, 84, 1484, 132, 236, 4, 138, 157, 11, 858, 272, 702, 763, 20, 263, 189, 4034, 173, 117, 8, 31, 1, 17, 172, 6920, 252, 28, 372, 24, 10, 3932, 253, 2, 37, 1, 162, 94, 23, 3130, 2, 726, 181, 38, 959, 210, 108, 211, 2, 114, 1165, 7, 379, 134, 23, 232, 62, 149, 86, 7, 3119, 41, 119, 3, 12, 8571, 264, 167, 110, 16, 9, 32, 757, 193, 4834, 1594, 2, 79, 4, 1542, 763, 30, 136, 150]\n"
     ]
    }
   ],
   "source": [
    "# to create input-target pairs (= word-contexts pairs) we use \n",
    "# tf's continuous skipgram model \n",
    "# https://www.tensorflow.org/tutorials/text/word2vec\n",
    "\n",
    "bible_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      tokenized_data,\n",
    "      vocabulary_size=VOCAB_SIZE,\n",
    "      window_size=WINDOW_SIZE,\n",
    "      negative_samples=0)\n",
    "\n",
    "bible_skip_grams = [[vocab[x[0]], vocab[x[1]]] for x in bible_skip_grams]\n",
    "inputs, targets = map(list, zip(*bible_skip_grams))\n",
    "print(targets[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f1179a-e6ee-4681-85dd-32f5f5e63ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([ 167,   40,   30, 1883,   27,  323,   28,  123,   60,   76, 1167,\n",
       "         214, 1514,  141,   27,  223,  700,  150,   69, 1550,   81,  908,\n",
       "          42,  868,   62,  116,  135,    1,  332,  580,    1,  833])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([ 701,    2,  142,    3,  513,    1,   55, 5483,   96,    1,  129,\n",
       "          64,   18,  493,    2,   16,   11,   29,    7,   16,  236,   94,\n",
       "          13,   10,  327,   19,   33,    2,   13,   38,    2,   91])>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the usual cache, shuffle, batch, prefetch\n",
    "bible_dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(1000).batch(32).take(10000)\n",
    "\n",
    "# print how a batch looks like\n",
    "iterator = iter(bible_dataset)\n",
    "iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d247d-9204-4bdc-9d19-a05c87ea07fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a69766-f8a2-4d88-a3d1-a186c17bf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, optimizer, embedding_size):\n",
    "        super(SkipGramModel,self).__init__()\n",
    "        \n",
    "        self.vocabulary_size = VOCAB_SIZE\n",
    "        self.embedding_size = embedding_size\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name = \"loss\")\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def build(self,string):\n",
    "        self.embedding_mat = self.add_weight(shape=(self.vocabulary_size, self.embedding_size),\n",
    "                                             initializer=\"uniform\",\n",
    "                                             trainable=True) \n",
    "        self.output_mat = self.add_weight(shape=(self.vocabulary_size, self.embedding_size),\n",
    "                                          initializer=\"random_normal\",\n",
    "                                          trainable=True) \n",
    "        self.output_bias = self.add_weight(shape=(self.vocabulary_size,),\n",
    "                                           initializer=\"zero\",\n",
    "                                           trainable=True)\n",
    "    def call(self, inputs):\n",
    "        target_predicted = tf.nn.embedding_lookup(params=self.embedding_mat, ids=inputs)\n",
    "        return target_predicted\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "    \n",
    "    def train(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.nce_loss(weights=self.output_mat,\n",
    "                               biases=self.output_bias, \n",
    "                               labels=tf.expand_dims(targets,axis=1), \n",
    "                               inputs=predictions,\n",
    "                               num_sampled=1,\n",
    "                               num_classes=self.vocabulary_size))\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ecd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    cos_sim = np.dot(A,B)/(norm(A)*norm(B))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "875d2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nearest_neighbours(tracked_words):\n",
    "    \n",
    "    for word in tracked_words:\n",
    "    \n",
    "        word_id = vocab[word]\n",
    "        # calculate all cosine similarity scores between the word's embedding vector and \n",
    "        # the emb vectors of all other words in the vocab\n",
    "        cos_sims = [cosine_similarity(model.call(word_id), model.call(word_ids)) for word_ids in range(VOCAB_SIZE)]\n",
    "        # get the n = 5 largest cos_sim values\n",
    "        nlarg = nlargest(5, cos_sims)\n",
    "        # get the indices of the highest cos_sim values\n",
    "        idx_of_nearest_neigh = [np.where(cos_sims == nlarg[i]) for i in range(len(nlarg))] \n",
    "        # unnest the result to have a nice array of indices\n",
    "        idx_of_nearest_neigh = [idx_of_nearest_neigh[i][0][-1] for i in range(len(idx_of_nearest_neigh))]\n",
    "        # look up the words that belong to the indices \n",
    "        nearest_neigh = [inverse_vocab[idx] for idx in idx_of_nearest_neigh]\n",
    "        \n",
    "        print(word, \"- nearest neighbours: \", nearest_neigh)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd2e0d1c-4273-497b-9ebb-1d1ec842caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_ds, epochs, summary_writer):\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch: ', epoch)\n",
    "\n",
    "        for data in train_ds:\n",
    "            metrics = model.train(data)\n",
    "\n",
    "        # log train loss\n",
    "        with summary_writer.as_default():  \n",
    "            # for scalar metrics:\n",
    "            for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "        \n",
    "        print_nearest_neighbours(tracked_words)\n",
    "       \n",
    "        model.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f61a7fc-8435-455a-9128-e7b8d831b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_writer(config_name):\n",
    "    \n",
    "    # Define where to save the logs\n",
    "    # along with this, you may want to save a config file with the same name so you know what the hyperparameters were used\n",
    "    # alternatively make a copy of the code that is used for later reference\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "\n",
    "    # log writer for training metrics\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "    \n",
    "    return train_summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13f3354b-95ef-40dd-812c-06586847b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "['loss: 5.883515357971191']\n",
      "holy - nearest neighbours:  ['holy', 'hearts', 'brought', 'been', 'jacob']\n",
      "water - nearest neighbours:  ['water', 'twelve', 'wisdom', 'obey', 'six']\n",
      "wine - nearest neighbours:  ['wine', 'hearts', 'more', 'far', 'three']\n",
      "love - nearest neighbours:  ['love', 'six', '29', 'power', 'tabernacle']\n",
      "son - nearest neighbours:  ['son', 'israel', 'out', 'house', 'children']\n",
      "father - nearest neighbours:  ['father', 's', 'over', 'eyes', 'jacob']\n",
      "devil - nearest neighbours:  ['devil', 'master', 'hour', 'lie', 'present']\n",
      "Epoch:  1\n",
      "['loss: 4.939197063446045']\n",
      "holy - nearest neighbours:  ['holy', 'who', 'way', 'truth', 'days']\n",
      "water - nearest neighbours:  ['water', 'twelve', 'jeroboam', 'pitched', 'wall']\n",
      "wine - nearest neighbours:  ['wine', 'houses', 'thirty', 'honey', 'pitched']\n",
      "love - nearest neighbours:  ['love', '24', 'art', '9', 'found']\n",
      "son - nearest neighbours:  ['son', 'house', 'children', 'tabernacle', 'israel']\n",
      "father - nearest neighbours:  ['father', 'head', 'praise', 'babylon', '36']\n",
      "devil - nearest neighbours:  ['devil', 'just', 'eastward', 'builded', 'worthy']\n",
      "Epoch:  2\n",
      "['loss: 4.484419822692871']\n",
      "holy - nearest neighbours:  ['holy', 'morning', 'priest', 'way', 'at']\n",
      "water - nearest neighbours:  ['water', 'twelve', 'lay', 'understanding', 'choose']\n",
      "wine - nearest neighbours:  ['wine', 'breadth', 'darkness', 'most', 'rod']\n",
      "love - nearest neighbours:  ['love', 'yea', '40', 'how', 'another']\n",
      "son - nearest neighbours:  ['son', 'house', 'children', 'king', 'all']\n",
      "father - nearest neighbours:  ['father', '36', 'head', 'mother', 'joseph']\n",
      "devil - nearest neighbours:  ['devil', 'borne', 'eastward', 'low', 'fifteen']\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "EPOCHS = 3\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model = SkipGramModel(optimizer, 64)\n",
    "summary_writer = create_summary_writer(\"model1\")\n",
    "training_loop(model, train_ds=bible_dataset, epochs= EPOCHS, summary_writer=summary_writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "iannwtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
